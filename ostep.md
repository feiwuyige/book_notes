[TOC]

# OSTEP

## 第一部分 虚拟化

### 第四章 抽象：进程

1. 理解进程的关键在于理解它的**机器状态（state machine）**，即程序在运行时可以读取或更新的内容。
   * 内存：程序的指令是存放在内存中的，进程读取和写入的数据也在内存中。进程可以访问的内存称为**地址空间（address space）。**
   * 寄存器：PC指针，栈基址 rbp，栈顶指针 rsp
   * 磁盘：程序可能会修改磁盘中的内容，比如当前程序打开的文件。
2. 进程api：操作系统是对硬件的抽象，同时在应用视角应该是给应用程序提供服务，也就是 api，所以显然操作系统中应该有一组处理进程的 api：
   * 创建进程(create)：创建一个新的进程。
   * 销毁进程(destroy)：如果进程不能正常退出，应当可以关闭。
   * 等待进程(wait)：有的时候进程之间需要同步，所以可能需要某一个进程等待另一个进程完成。
   * 其他控制(miscellanous control)：比如可能将某个进程暂时挂起，然后再恢复。
   * 状态(statu)：获取当前进程的相关信息，比如占用了多少内存。
3. 进程创建的步骤：
   * 程序的指令和数据只有在内存中才可以执行，而程序就是一个存放在磁盘上的文件，所以**首先一定要先将程序从磁盘加载到内存中**。
   * 为程序的运行时栈分配内存。
   * 为程序的堆分配一些内存。
   * 其他初始化任务，比如打开进程的文件描述符，将0 1 2绑定到标准输入、标准输出、标准错误输出。
   * 启动程序，将 CPU 的控制权转移到新创建的进程中去。
4. 进程的状态：
   * 运行：正在占用 CPU
   * 就绪：等待调度，可以去 CPU 上执行指令
   * 阻塞：因为某些原因需要等待某些操作完成，比如 I / O，此时不需要 CPU ，所以让 CPU 调度其他程序，从而提高效率。

### 第五章 插叙：进程API

1. * `fork`
   * `execve`
   * `waitpid`
   * `_exit`

### 第六章 机制：受限直接执行

1. 通过对 `CPU` 进行时分共享来实现虚拟化，即运行一个进程一段时间以后切换为另一个进程进行运行。要实现这样的机制有如下挑战：

   * 不增加系统开销，实现高性能
   * 对 `CPU` 的控制权，操作系统对资源进行管理，所以理应具有资源的控制权

2. 为了保证性能，所以采用**直接执行**的方式，就是将进程直接放在物理 `CPU` 上进行程序执行，这样的优势是**快速**，但是进程肯定要进行相关的其他操作，比如I/O，这些操作应当是有限制的，换句话说，应当由操作系统来进行管理，比如进程能否访问这个文件，所以硬件通过提供不同的执行模式来协助操作系统。

   * 用户模式：应用程序不能完全访问硬件资源。
   * 内核模式：操作系统可以访问机器的全部资源，并且提供陷入内核和从陷阱返回到用户模式程序的方式。

   当用户希望执行特权操作时，我们可以使用**系统调用**，他允许内核向用户程序暴露某些关键功能。要执行系统调用，程序必须执行特殊的陷阱（trap）指令，该指令跳入内核同时将特权级别提升到内核模式。执行完毕后，操作系统调用一个特殊的从陷阱返回（return-from-trap）指令，该指令返回到发起调用的用户程序中，同时将特权级别降低，回到用户模式。

3. 内核通过在启动时设置陷阱表（trap table）来实现陷阱指令的跳转，即告诉硬件在发生某些异常事件时要运行哪些代码。一旦硬件被通知，它就会记住这些处理程序的位置，直到下一次重新启动机器，并且硬件知道在发生系统调用和其他异常事件时要做什么（即跳转到哪段代码）。
4. 为了进行进程切换，操作系统一定要获取 `CPU` 的控制权，也就是说必须在 `CPU` 上执行操作系统的代码，但是此时用户的进程正在执行，所以有以下机制来达到目标：
   * 协作方式：等待系统调用，进程执行系统调用时，将 `CPU` 的控制权交给操作系统。或者应用程序执行某些非法操作。（死循环，不调用系统调用怎么办？
   * 非协作方式：操作系统进行控制，利用时钟中断使得操作系统重新获取 `CPU` 的控制权。（中断向量表，在启动时告诉硬件哪些代码在发生时钟中断时运行）
5. 无论以哪一种方式，当操作系统获得 `CPU` 的控制权时，将使用调度程序来决定运行哪一个进程，如果决定进行切换，操作系统就会进行**上下文切换**，即为当前正在执行的进程保存一些寄存器的值到内核栈，并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。
6. 中断处理执行程序在执行时、或者系统调用执行时发生了中断怎么办？操作系统应该设计类似的机制来进行处理，即**并发**模块。

### 第七章 进程调度：介绍

1. 要思考的关键问题：
   * 如何开发一个考虑调度策略的基本框架？
   * 有哪些关键假设？
   * 哪些指标非常重要？
   * 哪些基本方法已经在早期的系统中使用？
2. 调度指标：
   * 周转时间：$$T_{周转时间} = T_{完成时间} - T_{到达时间}$$
   * 响应时间：$$T_{响应时间} = T_{首次运行} - T_{到达时间}$$
   * 公平性：是否每一个进程在调度的过程中被公平对待。
3. 调度算法：
   * `FIFO`：可能会出现护航效应，一些好事较少的潜在资源消费者被排在重量级的资源消费者之后。即前面的任务需要很长时间才能执行完毕，从而导致整体的周转时间变多。
   * `SJF` ：先运行最短的任务，然后是次短的任务，如此下去。但是任务肯定不会同时到达，而是随即到达，该策略不允许抢占，那么也会出现护航效应。
   * `STCF`：最短完成时间优先，即向 `SJF` 添加抢占。
   * `RR` ：轮转调度，在一个时间片内运行一个工作，然后切换到运行队列中的下一个任务。时间片长度必须是**时钟中断周期的倍数**。

### 第八章 调度：多级反馈队列

1. 实际上，我们对于进程的执行时间一无所知，所以我们需要在没有工作长度的先验知识的前提下，设计一个能同时减少响应时间和周转时间的调度程序。**所以我们要从历史中学习，利用历史去预测未来**。计算机系统中还有很多地方是这样的思想，比如硬件的分支预测，缓存算法等。
2. MLFQ调度算法中存在很多队列，每个队列有不同的优先级，同一时间一个工作只能存在于一个优先级队列中，它的基本规则：

   * 运行优先级较高的队列中的工作
   * 同一优先级队列中的工作，使用轮转的方式进行运行

   所以该调度算法的关键在于优先级的确定，它采用一种从历史学习的方法，比如如果一个进程不断的放弃CPU去等待键盘输入，那么该进程可能会是一个交互程序，需要响应时间，所以其优先级较高；如果一个进程一直占用CPU而没有其他操作，那可能它的优先级就会很低。

3. 由于是从历史过程去预测未来，所以我们一定要有某些机制，如：

   * 改变一个进程的优先级：我们使用的策略是当一个进程加入时，总是在优先级最高的队列，如果它的时间片用完，则进入下一个低优先级的队列，如果在它的时间片用完之前主动放弃CPU（可能是一个交互性工作，保证响应时间），则使他的优先级保持不变。（如果采用这样的策略，低优先级队列中的进程可能永远得不到执行；还有一些程序愚弄调度程序，使用99%时间片以后主动释放CPU从而使得自己一直在一个较高的优先级）
   * 为了解决**饥饿**问题，我们可以周期性提升所有工作的优先级，即经过一段时间以后，就将系统中的所有工作都加入到高优先级队列，该策略的关键就在于周期性时间的确认，确认一个合适的时间十分重要。
   * 为了避免愚弄调度程序，可以使用一个更好的计时方式，我们给每一个优先级队列中的进程都分配固定的时间配额，一旦用完了该时间配额，不管主动放弃了多少次，都将其加入到一个低优先级队列中。

### 第九章 调度：比例份额

1. 基本思想是：调度程序的最终目标是确保每一个工作可以获得一定比例的CPU时间，而不是优化周转时间和响应时间。所以关键在于如何设计调度程序来按照比例分配CPU。
2. 彩票调度：比如两个进程A和B，A拥有75张彩票，B拥有25张彩票，那么我们就希望A占75%的CPU时间，B占25%的CPU时间，我们通过产生一个1-100的随机数，如果随机数在1-75运行A否则运行B来做到这一点。
3. 步长调度：根据每个工作所占的份额来求取一个步长，再对于每一个工作记录它们的行程值，每次选择最小的行程值进程，然后进行执行，执行完加上一个它自己的步长。

### 第十章 多处理器调度（高级）

1. 缓存一致性：每个CPU都有自己的缓存，如果一个CPU上的进程更新了数据还没有写入内存，而另一个CPU上的进程就进行读取就会出现缓存不一致问题。**硬件提供了这个问题的基本解决方案，通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性**。比如在基于总线的系统中，一种方式是使用总线窥探（bus snooping）。每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果CPU发现对它放在缓存中的数据的更新，会作废（invalidate）本地副本（从缓存中移除），或更新（update）它（修改为新值）。
2. 同步：跨CPU访问，尤其是写入共享数据或数据结构时，需要使用互斥原语来保证正确性，比如锁。
3. 缓存亲和度：一个进程在CPU上运行会缓存一些数据，如果下次还在这个CPU上就会执行的很快，如果在其他的CPU上就执行的很慢。
4. 两种基本模式：
   * 单队列调度：将所有的进程放在一个队列中，然后依次放入不同的CPU中进行执行。这种方式对于共享资源的访问（队列）会出现竞争，可扩展性较差，且每个任务在哪个CPU 上执行是随机的，缓存亲和度较差。
   * 多队列调度：每一个CPU都有一个自己的调度队列，然后将这个调度队列中的任务放在对应的CPU上进行执行。可扩展性较好，缓存亲和度好，但是容易出现负载不均衡，比如某个CPU上的任务执行完毕了，那么这个CPU上就没有任务在执行，浪费了资源，可以使用**任务窃取**来从其他队列中把一些任务移动到该队列来避免该问题，但是什么时候去观察其他队列中的任务情况需要进行斟酌。
5. Linux多处理器调度程序：
   * O(1)调度程序：多队列调度，每个CPU维护140个优先级队列，每个队列对应一个优先级。（MLFQ，多级队列反馈）
   * 完全公平调度程序（CFS）：使用红黑树和虚拟运行时间跟踪，所有可运行任务按照虚拟运行时间排序插入红黑树，调度器始终选择虚拟运行时间最小的任务。（比例份额，步长调度）
   * BFS调度程序（BFS）：所有CPU使用一个全局任务队列，任务按照优先级插入队列，调度器选择最高优先级的任务分配给空闲CPU。

### 第十三章 抽象：地址空间

1. 地址空间（address space）：是运行的程序看到的系统中内容，是对物理内存的抽象。一个进程的地址空间包含运行的程序的所有内存状态，比如程序的代码、栈（保存函数调用信息、分配空间给局部变量、传递参数和函数返回值）、堆（管理动态分配的、用户管理的内存）、其他（静态初始化的变量）。
2. 虚拟内存系统的目标：
   * 透明：让运行的程序看不见虚拟内存系统的存在。
   * 效率：在空间和时间上尽可能高效。
   * 保护：确保进程受到保护，不会被其他进程影响。

### 第十四章 插叙：内存操作API

1. 在运行一个 C 程序的时候，会有两种内存类型的分配：
   * 栈：申请和释放由编译器进行管理。
   * 堆：由程序员进行显式的分配和回收。
2. `malloc` 调用：传入要申请的堆空间的大小，成功就返回一个指向新申请空间的指针，失败就返回`NULL`。
3. `free` 调用：接受一个由 `malloc` 返回的指针，不需要分配区域的大小，**必须由内存分配库本身记录追踪**。
4. 常见错误：
   * 忘记分配内存，即使用的指针并没有指向一块动态分配的内存区域。
   * 没有分配足够的内存。
   * 忘记初始化分配的内存。
   * 忘记释放内存
   * 在用完之前释放内存
   * 重复释放内存
   * 错误的调用 `free`，即给 `free` 传入的参数不是一个 `malloc` 得到的指针。
5. 系统中实际存在两级内存管理：
   * 第一级由操作系统执行的内存管理，操作系统在进程运行时将内存交给进程，并在进程退出时将其回收。
   * 第二级管理在每个进程中，例如调用 `malloc()` 和 `free()` ，在堆内管理内存。
6. 管理内存的工具：`purify`  `valgrind`。

### 第十五章 机制：地址转换

1. 基于硬件的地址转换，动态重定位，基址加界限机制：每个 `CPU` 需要两个硬件寄存器，**基址寄存器**和**界限寄存器**，使用这种方式，编写和编译程序时假设地址空间从零开始，但是当程序真正运行时，操作系统会决定其在物理内存中的实际加载地址，并将起始地址记录在基址寄存器中。

   > physical address = virtual address(address in program) + base

   界限寄存器确保这个地址在进程地址空间的范围内，从而来确保安全。

2. 操作系统必须记录哪些空闲内存没有使用，以便能够为进程分配内存，有许多数据结构可以用于这项任务，比如**空闲列表（free list）。**

3. 采用动态重定位技术，很容易产生内部碎片，即已经分配的内存单元内部有未使用的空间（即碎片）。（因为此时假设地址空间放在固定大小的槽块中）。

### 第十六章 分段

1. 我们之间假设将所有进程的地址空间完整加载到内存中去，然后通过简单的基址、界限寄存器来进行地址重定位，这样就造成了物理内存空间的浪费，因为堆与栈之间有大量的一些空闲内存，而这些内存无法被其他进程进行使用，造成了浪费。所以采用**分段**的思想，在典型的地址空间里有3个逻辑不同的段：**代码、栈、堆**。我们可以将进程逻辑不同的段放入不同的物理内存段，从而避免栈与堆之间浪费大量的内存空间，所以MMU里面则需要3对基址、界限寄存器。

2. 硬件在进行地址转换的时候，如何知道该地址属于哪个段？如何知道段内的偏移？

   * 显式方式：利用虚拟地址的开头即为来标识不同的段
   * 隐式方式：通过地址产生的方式来判断，比如，如果地址由程序计数器产生，那么地址在代码段，如果基于栈或基址指针，则在栈段，否则在堆段。

3. 当使用分段的方式进行重定位的时候，会出现一些**外部碎片**，分配的内存之间的有一些很小的内存空间没有得到使用，比如系统现在想分配一个20kb的段，系统的内存空间加起来有24kb，但是因为不连续，所以无法进行内存分配，所以可以有以下解决方案：

   * 紧凑物理内存：重新安排原有的段，让已分配的物理内存紧凑起来，但是该操作是内存密集型任务，会占用大量的处理器时间。
   * 利用空闲列表管理算法，试图保留大得的内存块用于后续的分配，但是算法只能减少，不能避免。

4. 分段机制总结：
   好处：

   * 更高效的虚拟内存，避免了内部碎片。
   * 代码共享，可以给一些段加上控制权限，从而来实现共享。

   坏处：

   * 外部碎片
   * 还是无法解决稀疏地址空间，比如有一个很大但是很稀疏的堆，它们在同一个逻辑段里面，整个堆还是必须完整的加载到内存中。

### 第十七章 空闲空间管理

1. 当需要管理的空间被划分为固定大小的单元，就很容易进行管理，但是如果要管理的空闲空间由大小不同的单元构成，管理就变得困难，主要要考虑以下问题：
   * 要满足变长的分配请求，应该如何管理空闲空间？
   * 什么策略可以让碎片最小化？
   * 不同方法的时间、空间开销如何？

2. 要提供的机制：
   * 分割：当用户申请的字节数小于空闲列表的长度时，要将该空闲列表进行分割，一部分返回给用户，一部分留着等待后续使用。
   * 合并：当用户归还内存的时候，不应该只是简单的将该内存加入到空闲列表中，而应当与相邻的内存块进行合并。
   * 追踪已分配空间的大小：在 `free` 函数中并没有传入大小，所以内存分配库要能确定要释放空间的大小，从而将其放回空闲列表。大多数分配程序会在头块中保存一点额外的信息，比如用户要20字节的内存，则会先在内存区域存放相关的数据结构，再将该内存后面的20字节返回给用户，释放的时候则可以通过前面的头块数据来读取相关长度。（做实验发现glibc中malloc中要求8字节对齐，最小分配24个字节，可以使用malloc_usable_size()函数返回实际分配的空间大小，而分配的内存前面16个字节则是头块数据结构，两个size_t类型，因为要求8字节对齐，所以分配的长度肯定是8的倍数，且不少于24个字节，所以64位的最低4位用作标志位）。
   * 嵌入空闲列表：在典型的列表实现中，我们使用 `molloc` 来分配一个节点所需要的内存空间，但是当我们使用一个空闲列表来管理 `malloc` 分配的内存时，则需要在空闲空间本身建立空闲空间列表，通过 `mmap` 系统调用来实现，该系统调用返回一个指针，指向一块空闲内存。
   * 让堆增长：大多数传统的分配程序会在堆空间不足时，向操作系统申请更大的空间，使用 `sbrk` 系统调用，会找到空闲的物理内存页，将他们映射到进程的地址空间中，并返回新的堆的末尾地址，从而增长堆空间的大小。

3. 管理空闲空间的基本策略：

   * 最优匹配：遍历空闲列表，分配大于等于用户要求空间大小的最小空闲内存。
   * 最差匹配：找到最大的空闲块，分割并且满足用户要求的空间大小以后，将剩余的块加入空闲列表。
   * 首次匹配：遍历空闲列表，分配第一个满足用户要求的内存块给用户。
   * 下次匹配：在首次匹配的基础上，维护一个额外的指针，记录上次分配的位置，然后下次分配时从该位置进行遍历。

4. 《Understanding glibc malloc》
   1. `brk` 



### 第18章 分页：介绍
1. 解决空间管理问题：
   * 将空间分割成不同长度的分片，分段。
   * 将空间分割成固定长度的分片，分页。
2. 对于每一个进程，操作系统都需要去记录该进程的每一个虚拟页存放在物理内存中的位置，所以操作系统为每一个进程设计了一个数据结构，称为页表（page table）。页表的作用就是保存虚拟页面与物理页面之间的转化关系，也就是一个虚拟页对应的是哪一个物理页。
3. 虚拟地址：虚拟页面号（virtual page number, VPN）和页内偏移（offset，页的大小即确定便宜的位数）
4. 页表的组织：页表就是一种数据结构，用于将虚拟地址映射到物理地址，比如可以是一个数组，os通过虚拟页号VPN检索数组，找到页表项pte（page table entry），然后再找到对应的物理帧号。除了这个简单的对应关系以外，页表项中还包含：
   * 有效位（valid bit）：用于标记地址转换是否有效，os并不会刚开始就给每个虚拟内存分配一个物理页面，而是可以将一些页面标记为无效，这样等os访问的时候，就会进入os代码，去判断访问是否合法，再去进行内存的分配或者 `segmentation fault`。
   * 保护位（protection bit）：用于标记页面是否可以读取、写入、执行。
   * 存在位（present bit）：用于标记该页面是在物理存储器还是在磁盘上，即这一页面是否已经换出（swapped out），从内存换到磁盘。
   * 脏位（dirty bit）：表明该页被带入内存以后是否进行过修改，用于实现一致性。
   * 参考位（reference bit）：用于追踪页是否被访问，用于确定哪些页受欢迎，因此应该保存在内存中。
5. 要进行地址转换，硬件必须知道当前正在运行的进程的页表的位置。


### 第19章 分页：快速地址转换（TLB）
1. 使用分页机制来实现虚拟内存，就需要将地址空间切分成大量固定大小的单元，并且需要记录这些单元的映射信息，而且这些映射信息一般存储在物理内存中，所以在进行虚拟地址转换的时候，分页逻辑上需要一次额外的内存访问。（去内存空间读取到映射关系，然后再进行内存访问。）
2. 为了加快内存访问，os则需要**硬件**的帮助，即地址转换旁路缓冲存储器（translation-lookaside buffer,TLB），频繁发生的虚拟地址到物理地址转换的硬件缓存。对于每次内存访问，硬件先检查TLB，如果有期望的映射，则不再需要去访问页表。
3. 缓存是计算机系统中提升性能的一个常见手段，其基本思路是利用程序的时间局部性和空间局部性原理。但是当 `TLB` 没有命中的时候，必须有一个东西来处理这种情况，很显然，有以下两种选择：
* 硬件：一般是 cisc 体系架构计算机，有更复杂的指令集。
* 操作系统：risc 体系架构，当 `TLB` 未命中的时候，硬件系统抛出异常，操作系统暂停当前的执行流，提升至内核模式，跳转至对应的陷阱处理程序，也就是说，处理未命中的代码是一段 `os` 代码。
4. 当利用操作系统处理 `TLB` 未命中的时候，有以下细节需要注意：
* 不同于一般的系统调用，当从陷阱处理程序返回以后，执行陷入操作系统之后的那条指令，但是 `TLB` 未命中从系统调用返回以后，应当重复执行刚才未命中的那条指令，所以在陷入 `os` 内核的时候，要根据情况保存不同的 `pc`。
* 要避免无限递归，即陷阱处理程序中也会 `TLB` 未命中。要解决这一问题，可以把陷阱处理程序直接放在物理内存中去，或者在 `TLB` 中保留一些项，记录永远的地址转换，并且将一些项留给陷阱处理程序本身的代码块。
5. `TLB` 中包含的虚拟到物理地址的映射只对当前进程有效，对其他进程是没有意义的。**在上下文切换时，操作系统必须改变页表基址寄存器（PTBR）的值。`PTBR` 中存放的是当前进程页表的物理地址。**


### 第20章 分页：较小的表
1. 每个进程都有自己的页表，如果页表的大小太大，当系统中运行多个进程的时候，那么页表占用的内存就过大了。


### 第20章 分页：较小的表
1. 假设32位地址空间，每个页大小为 4 KB，那么一共有 $2^{32} / 2^{12} = 2^{20} $ 个页表项，每一个页表项为 4 个字节，那么页表就要占 $2 ^ {20} \times 4 = 2 ^ {22} = 4 MB$，这对于内存空间负担太大，所以我们需要一些方法来使得页表的空间变小。一种方法就是缩小页表项的大小，一种方法就是减少页表项的数量，这是两种基本思路。
   * 更大的页：通过增加每个页的大小，在地址空间保持不变的情况下，就可以减少页表项的数量。但是这种方法会造成页内的浪费，因为将某个页加载到内存的时候，可能只使用一小部分。（内存页太大）
   * 分页和分段：我们将内存分为代码、堆、栈，然后在每个段中再进行分页，使用基址寄存器指向保存该段的页表的物理地址，界限寄存器用于指示页表的结尾。这种方法的主要思路在于减少内存中无效的页表，**我们不是为进程的整个地址空间提供单个页表，而是为每个逻辑分段提供一个。**
   * 多级页表：与分页和分段方法思路一样，即**去掉页表中的所有无效区域，而不是将他们都全部保留在内存中**。将页表分成页大小的单元，如果整页的页表项（PTE）无效，就不分配该页的页表。使用页目录来记录。
2. 超越二级页表：
假设地址空间30位，每一页大小是512个字节，也就是说页内偏移需要9位，页号一共则有21位，**构建多级页表的目标是为了让页表的每一部分都放入一个页，这样子就可以通过页目录项来进行索引。**我们假设每个页表项位4个字节，也就是说一页中可以放入128个页表项，那么我们索引页表的时候就需要 7 位（页表的页内偏移），此时还有14（30 - 9 - 7 = 14）位可以用来索引页表，也就是存放页目录项的表，我们的页目录项就一共有 2 ^ 14 个，也就是页目录项要占用 2 ^ 14 * 4 / 512 = 2 ^ 7 = 128 个页，还是很占用内存空间，所以可以再建立一级页表。
```
cpp
1    VPN = (VirtualAddress & VPN_MASK) >> SHIFT
2    (Success, TlbEntry) = TLB_Lookup(VPN)
3    if (Success == True)    // TLB Hit
4        if (CanAccess(TlbEntry.ProtectBits) == True)
5            Offset   = VirtualAddress & OFFSET_MASK
6            PhysAddr = (TlbEntry.PFN << SHIFT) | Offset
7            Register = AccessMemory(PhysAddr)
8        else
9            RaiseException(PROTECTION_FAULT)
10   else                  // TLB Miss
11       // first, get page directory entry
12       PDIndex = (VPN & PD_MASK) >> PD_SHIFT
13       PDEAddr = PDBR + (PDIndex * sizeof(PDE))
14       PDE     = AccessMemory(PDEAddr)
15       if (PDE.Valid == False)
16           RaiseException(SEGMENTATION_FAULT)
17       else
18           // PDE is valid: now fetch PTE from page table
19           PTIndex = (VPN & PT_MASK) >> PT_SHIFT
20           PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))
21           PTE     = AccessMemory(PTEAddr)
22           if (PTE.Valid == False)
23               RaiseException(SEGMENTATION_FAULT)
24           else if (CanAccess(PTE.ProtectBits) == False)
25               RaiseException(PROTECTION_FAULT)
26           else
27               TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
28               RetryInstruction()
```

### 第 21 章 超越物理内存：机制
1. 为什么要超越物理内存，给进程支持巨大的地址空间？
   * 方便和易用性：程序员只需要自然的编写程序，根据需要分配内存，而不必担心数据结构是否有空间进行存储。
   * 多道程序（能够同时运行多个程序，更好的利用机器资源）的出现强烈要求能够换出一些页，因为早期的机器显然不能将所有进程需要的所有内存页同时存放在内存中。
2. 为了使用硬盘上的交换页，我们需要引入一些新的机制，页表项中增加一个**存在位**来表示该页是否存在在物理内存中。
3. 当程序从内存中读取数据会发生什么？

### 第 22 章 超越物理内存：策略
1. 内存中只包含所有页的一个子集，所以可以将内存视为系统中虚拟内存页的缓存。
2. 常见策略：
* 最优策略（理想情况）：每次踢掉最远将来才会访问的页。
* FIFO：先进先出。不具有**栈特征**，增加缓存大小有可能命中率甚至下降。
* 随机：随机选择一个页面换出。
* LRU(最近最少使用) / LFU(最不经常使用)：利用历史信息来决定换出哪些页，一个是频率，如果一个页被访问了很多次，那么他不应该被换出；一个是近期性，越近被访问过的页，也许再次访问的可能性也就越大。

### 第 23 章 VAX/VMS虚拟内存系统
1. 页的按需置零（demand zeroing）：初级实现中，操作系统响应一个请求，在物理内存中找到页，将该页添加到堆中，并将其置零（否则你可以看到其他进程在使用这个页时的内容），然后将其映射到你的地址空间。利用按需置零，页加入地址空间时，os会在页表中放入一个标记页不可访问的条目，如果进程读取或写入页，会向os发送一个陷阱，此时os再进行置零，并且映射到进程的地址空间，从而减少开销，避免一些不会使用的页也进行按需置零。
2. 写时复制(copy on write)：如果os需要将一个页面从一个地址空间复制到另一个地址空间，不是实际复制它，而是将其映射到目标地址空间，并在两个地址空间中将其标记为只读，如果其中一个地址空间确实尝试写入页面，就会陷入os，os此时再分配一个新页面，填充数据。

### 第 26 章 并发：介绍

1. 线程：为当个运行进程提供的抽象。经典观点是一个程序只有一个执行点（一个ip寄存器，用来存放要执行的指令地址），但是多线程程序会有多个执行点（多个pc，每个都用于取指令和执行）。换一个角度看，每一个线程类似于独立的进程，只有一点区别：**线程之间共享地址空间，从而可以访问相同的数据。**每个线程有自己的一组用于计算的寄存器，所以在一个处理器上切换线程，就必须要保存相关信息。对于进程，我们将状态保存到进程控制块（PCB），而对于线程，我们需要一个或多个线程控制块（TCB）。但是，与进程相比，线程之间的上下文切换有一点主要区别：**地址空间保持不变（即不需要切换当前使用的页表）**，进程与线程之间的另一个主要区别在于栈，多线程程序中每一个线程都有一个栈。
2. 原子性：全部或没有。
3. 线程之间的交互：
* 访问共享变量：互斥，为临界区支持原子性。
* A线程需要在B线程执行完以后运行：同步。

### 第 27 章 插叙：线程API
1. 将 `void` 指针作为函数的参数，允许我们传入任何类型的参数；将 `void` 作为返回值，允许线程返回任何类型的结果。
2. `POSIX` 线程提供的锁的重要api：
```c
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_unlock(pthread_mutex_t *mutex);
//所有的锁在使用的时候都必须正确初始化
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_init(&lock, NULL);
//用完锁进行摧毁
pthread_mutex_destroy();
```
3. **要使用条件变量，必须另外有一个与此条件相关的锁。**因为条件变量也是共享资源，是多个线程可能同时访问的东西，所以要加锁保证互斥。
```c
//调用线程进入休眠状态，等待其他线程发出信号。
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
//唤醒某个条件。
int pthread_cond_signal(pthread_cond_t *cond);
```

### 第 28 章 锁